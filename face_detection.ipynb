{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### load image\n",
    "img = cv2.imread('images/afghan-girl.jpg')      # load image\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    # create a grayscale of the image and save it  as gray\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)      # convert image to RGB and save it\n",
    "\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(20,10))       # change the figure size to 20 by 10\n",
    "plt.imshow(img)   # display\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load cascade\n",
    "Create a variable with the location of the haarcascades file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "faceCascadePath = 'haarcascades/haarcascade_frontalface_default.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier(faceCascadePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(faceCascade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`faceCascade` is a CascadeClassifier\n",
    "\n",
    "`detectMultiScale` is a mehod within CascadeClassifier\n",
    "- its parameters are:\n",
    "    - **scaleFactor**: How much the image size is reduced at each image scale. This value is used to create the scale pyramid in order to detect faces at multiple scales in the image (some faces may be closer to the fore-ground, and thus be larger; other faces may be smaller and in the background, thus the usage of varying scales). A value of 1.05 indicates that Jeremy is reducing the size of the image by 5% at each level in the pyramid.\n",
    "    \n",
    "    - **minNeighbors**: How many neighbors each window should have for the area in the window to be considered a face. The cascade classiﬁer will detect multiple windows around a face. This parameter controls how many rectangles (neighbors) needto be detected for the window to be labeled a face.\n",
    "    \n",
    "    - **minSize**: A tuple of width and height (in pixels) indicating the minimum size of the window. Bounding boxes smaller than this size are ignored. It is a good idea to start with (30, 30) and ﬁne-tune from there.\n",
    "\n",
    "\n",
    "```python\n",
    "faceRects = faceCascade.detectMultiScale(gray, scaleFactor = 1.1, minNeighbors=5, minSize = (30,30))\n",
    "\n",
    "for (x,y, w, h) in faceRects:\n",
    "    cv2.rectangle(img, (x,y), (x+ w, y + h), (0,255,0), 2)\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(20,10))       # change the figure size to 20 by 10\n",
    "plt.imshow(img),plt.axis(\"off\")   # display and remove  x&y axis\n",
    "plt.show()                        # display image onto jupyter notebook cell\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "faceRects = faceCascade.detectMultiScale(gray, scaleFactor = 1.1, minNeighbors=5, minSize = (30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "faceRects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### face location\n",
    "`faceRects` is a numpy array with 4 values:\n",
    "- [[233, 108, 285, 285]]\n",
    "- 233 is the x location\n",
    "- 108 is the y location\n",
    "- 285 is the w location\n",
    "- 285 is the h location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (x,y, w, h) in faceRects:\n",
    "    cv2.rectangle(img, (x,y), (x+ w, y + h), (0,255,0), 2)\n",
    "   \n",
    "\n",
    "plt.figure(figsize=(20,10))       # change the figure size to 20 by 10\n",
    "plt.imshow(img)                   # display \n",
    "plt.show()                        # display image onto jupyter notebook cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
